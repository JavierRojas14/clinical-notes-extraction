{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style()\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/datos_limpios_diagnosticos.csv\",\n",
    "                 sep=\";\", encoding=\"latin-1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Análisis de contenido de detalle de atención\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atencion_prueba = df[\"detalle_atencion\"][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacion de Oraciones y Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza por oracion y palabras\n",
    "separado_por_oracion = sent_tokenize(atencion_prueba)\n",
    "separado_por_palabra = word_tokenize(atencion_prueba)\n",
    "\n",
    "print(separado_por_oracion)\n",
    "print(separado_por_palabra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtro por StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"spanish\")\n",
    "atencion_sin_stopwords = [palabra for palabra in separado_por_palabra if palabra not in stop_words]\n",
    "print(atencion_sin_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was taking a ride in the car.\n",
    "# I was riding  in the car.\n",
    "# En este caso, ride y riding significan lo mismo. Con stemming se reduce a la raiz de una palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_prueba = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "for w in palabras_prueba:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_texto = (\n",
    "    \"It is very important to be pythonly while you are pythoning with python. \"\n",
    "    \"All pythoners have pythoned poorly at least once.\"\n",
    ")\n",
    "\n",
    "palabras = word_tokenize(nuevo_texto)\n",
    "for palabra in palabras:\n",
    "    print(ps.stem(palabra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech\n",
    "\n",
    "Agrega una etiqueta a cada palabra, indicando si la palabra es un articulo, sustantivo, adjetivo,\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CC: It is the conjunction of coordinating\n",
    "- CD: It is a digit of cardinal\n",
    "- DT: It is the determiner\n",
    "- EX: Existential\n",
    "- FW: It is a foreign word\n",
    "- IN: Preposition and conjunction\n",
    "- JJ: Adjective\n",
    "- JJR and JJS: Adjective and superlative\n",
    "- LS: List marker\n",
    "- MD: Modal\n",
    "- NN: Singular noun\n",
    "- NNS, NNP, NNPS: Proper and plural noun\n",
    "- PDT: Predeterminer\n",
    "- WRB: Adverb of wh\n",
    "- WP$: Possessive wh\n",
    "- WP: Pronoun of wh\n",
    "- WDT: Determiner of wp\n",
    "- VBZ: Verb\n",
    "- VBP, VBN, VBG, VBD, VB: Forms of verbs\n",
    "- UH: Interjection\n",
    "- TO: To go\n",
    "- RP: Particle\n",
    "- RBS, RB, RBR: Adverb\n",
    "- PRP, PRP$: Pronoun personal and professional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_con_tag = nltk.pos_tag(atencion_sin_stopwords)\n",
    "print(palabras_con_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkgram = r\"\"\"\"Chunk: {<NNP.?>+<NN.?>}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkgram)\n",
    "chunked = chunkParser.parse(palabras_con_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinking\n",
    "\n",
    "Indica todo lo que se quiere dejar fuera de los chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkgram = r\"\"\"\"Chunk: {<.*>+}\n",
    "                        }<VB.?|IN|DT>+{\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkgram)\n",
    "chunked = chunkParser.parse(palabras_con_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifica entidades dentro de un texto. Las entidades corresponden a organizaciones (Un Hospital),\n",
    "personas, fechas, porcentajes, localizaciones, entre otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades_en_texto = nltk.ne_chunk(palabras_con_tag, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades_en_texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "\n",
    "Es como el Stemming, pero las palabras finales son palabras reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui se puede ver que better se transformo a good, ya que es un adverbio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "\n",
    "Se pueden tomar palabras y buscar sinónimos y antónimos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se pueden buscar sinonimos de una palabra\n",
    "\n",
    "syns = wordnet.synsets(\"good\")\n",
    "print(syns)\n",
    "\n",
    "# Solo el nombre\n",
    "print(syns[0].lemmas())\n",
    "\n",
    "# Definicion de la palabra\n",
    "print(syns[0].definition())\n",
    "\n",
    "# Ejemplos\n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se pueden buscar antonimos de una palabra\n",
    "\n",
    "for sinonimo in syns:\n",
    "    for lema in sinonimo.lemmas():        \n",
    "        print(lema)\n",
    "        if lema.antonyms():\n",
    "            print(f\"   |----{lema.antonyms()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "Existen los unigramas (toma las palabras por si solo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"I love the book book\", \"This is a great book\", \"The fit is great\", \"I love the shoes\"]\n",
    "y_train = [\"BOOKS\", \"BOOKS\", \"CLOTHING\", \"CLOTHING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizador = CountVectorizer()\n",
    "resultado_vectorizado = vectorizador.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizador.get_feature_names_out())\n",
    "print(resultado_vectorizado.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = SVC(kernel=\"linear\")\n",
    "modelo.fit(resultado_vectorizado, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [\"This is such a bad book\", \"I like this shoes!\"]\n",
    "X_test_vectores = vectorizador.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.predict(X_test_vectores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrama\n",
    "\n",
    "Aqui toma conjuntos de dos palabras y las hace un vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizador = CountVectorizer(ngram_range=(1, 2))\n",
    "resultado_vectorizado = vectorizador.fit_transform(X_train)\n",
    "\n",
    "print(vectorizador.get_feature_names_out())\n",
    "print(resultado_vectorizado.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los problemas ocurre cuando el conjunto de prueba contiene una palabra que nunca apareció\n",
    "en el conjunto de entrenamiento. En tal caso, el modelo deja de tomar en cuenta esta palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word of Vectors\n",
    "\n",
    "Permite agrupar palabras que semánticamente son similares, por ejemplo: \"Rojo\", \"Azul\", \"Verde\" todos los\n",
    "agrupa en \"Color\". Otro ejemplo es: \"Best book I've read in year\", donde uno da cuenta de que \"read\"\n",
    "siempre aparece cerca de book. Debido a la relación anterior, se genera un vector en el espacio\n",
    "que agrupa estos términos juntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(text) for text in X_train]\n",
    "X_train_wv_vector = [X.vector for X in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representa cada oracion en muchos vectores\n",
    "print(docs[0].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train_wv_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [\"This is such an awesome book!\", \"This zapato is not that great\"]\n",
    "X_test_vectores = [nlp(text).vector for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.predict(X_test_vectores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = re.compile(r\"^ab[^\\s]*cd$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\"abcd\", \"xxxx\", \"abxxxxcd\", \"xxabcdxx\", \"abklasduoiasduoqwduocd\"]\n",
    "for phrase in phrases:\n",
    "    if re.match(regexp, phrase):\n",
    "        print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo_borde = [\"aaa xxabcdxx ccc\"]\n",
    "# for phrase in ejemplo_borde:\n",
    "#     if re.match(regexp, phrase):\n",
    "#         print(phrase)\n",
    "\n",
    "\n",
    "# Aqui se utiliza search en vez de match. Lo que hace es buscar en cada palabra de un conjunto de strings\n",
    "regexp = re.compile(r\"ab[^\\s]*cd\")\n",
    "ejemplo_borde = [\"aaa xxabcdxx ccc\"]\n",
    "for phrase in ejemplo_borde:\n",
    "    if re.search(regexp, phrase):\n",
    "        print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/Lemmatization\n",
    "\n",
    "Es una técnica para normalizar texto! Reduce todo a su forma basal. Lemmatizing garantiza que\n",
    "la palabra reducida tiene sentido, mientras que stemming puede que no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "phrase = \"Reading the books\"\n",
    "phrase_tokenized = word_tokenize(phrase)\n",
    "\n",
    "stemmed_words = [stemmer.stem(palabra) for palabra in phrase_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lema = WordNetLemmatizer()\n",
    "\n",
    "lema_words = [lema.lemmatize(palabra, pos=\"v\") for palabra in phrase_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lema_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo que es muy importante es que el lematizador espera que esté el Part of Speech de cada palabra.\n",
    "De esa forma, permite reducir de forma más precisa cada palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ingles = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtradas = [palabra for palabra in phrase_tokenized if palabra not in stopwords_ingles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtradas)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
